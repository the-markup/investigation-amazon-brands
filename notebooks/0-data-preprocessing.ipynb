{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This notebook will preprocess searchs, product pages, the our brands API, best sellers, and create our Amazon Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import Counter\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import parsers as P\n",
    "from tqdm import tqdm\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "\n",
    "from utils import value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "pattern_searches = '../data/input/search-selenium/*/*/*/2021/01/21/webpage_search.html'\n",
    "pattern_our_brands_api = '../data/input/search-private-label/*/*/*/2021/*/*/response__*.json'\n",
    "pattern_our_brands_html = '../data/input/search-selenium-our-brands-filter_/*/*/*/2021/*/*/webpage_search__*.html'\n",
    "pattern_our_brands_search = '../data/input/search-private-label/*/*/*/2021/*/*/webpage_ourbrands.html'\n",
    "pattern_best_sellers = '../data/input/best_sellers/*/*/*/*/page_*/2021/*/*/*.html'\n",
    "pattern_products = '../data/input/selenium-products/*/*/*/2021/02/*/webpage_product.html'\n",
    "fn_seller_central = '../data/input/seller_central/All_Q4_2020.csv.xz'\n",
    "\n",
    "# outputs\n",
    "data_dir = '../data/output/datasets'\n",
    "fn_amazon = f'{data_dir}/amazon_private_label.csv.xz'\n",
    "fn_search = f'{data_dir}/searches.csv.xz'\n",
    "fn_products = f'{data_dir}/products.csv.xz'\n",
    "fn_training_set = f'{data_dir}/training_set.csv.xz'\n",
    "\n",
    "fn_brands_api = f'{data_dir}/non_essential/our_brands_api.csv.gz'\n",
    "fn_brand_search = f'{data_dir}/non_essential/our_brands_search.csv.gz'\n",
    "fn_brands_filter = f'{data_dir}/non_essential/our_brands_filter.csv.gz'\n",
    "fn_best_sellers = f'{data_dir}/non_essential/best_sellers.csv.gz'\n",
    "\n",
    "os.makedirs(f\"{data_dir}/non_essential\" , exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search\n",
    "Parses raw search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12717"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_searches = glob.glob(pattern_searches)\n",
    "len(files_searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12717/12717 [01:09<00:00, 183.19it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_search):\n",
    "    search_data = []\n",
    "    with Pool(processes=32) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.process_search_result, files_searches), \n",
    "                           total=len(files_searches)):\n",
    "            search_data.extend(record)\n",
    "            \n",
    "    df_search = pd.DataFrame(search_data)\n",
    "    df_search.to_csv(fn_search, index=False, compression='xz')\n",
    "\n",
    "else:\n",
    "    df_search = pd.read_csv(fn_search, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_type\n",
       "editorial_recs_carousel      0.352660\n",
       "featured_brands_carousel     0.125199\n",
       "misc_carousel                0.028034\n",
       "misc_sponsored_carousel      0.020787\n",
       "regular_placement            0.998168\n",
       "regular_placement__missed    0.141048\n",
       "sponsored_banner             0.686684\n",
       "Name: search_term, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_search.groupby('product_type').search_term.nunique() / df_search.search_term.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our brands search\n",
    "parse search results made in the \"our brands\" department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_our_brands_search = glob.glob(pattern_our_brands_search)\n",
    "len(files_our_brands_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 795/795 [00:00<00:00, 965.58it/s] \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_brand_search):\n",
    "    files_our_brands_search = glob.glob(pattern_our_brands_search)\n",
    "    print(len(files_our_brands_search))\n",
    "\n",
    "    brands_search_data = []\n",
    "    with Pool(processes=32) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.process_search_result, \n",
    "                                               files_our_brands_search), \n",
    "                           total=len(files_our_brands_search)):\n",
    "            brands_search_data.extend(record)\n",
    "\n",
    "    df_brands_search = pd.DataFrame(brands_search_data)\n",
    "    df_brands_search = df_brands_search[~df_brands_search.is_sponsored]\n",
    "    df_brands_search.to_csv(fn_brand_search, index=False, compression='gzip')\n",
    "    \n",
    "else:\n",
    "    df_brands_search = pd.read_csv(fn_brand_search, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our brands API\n",
    "parses API responses that mimic the \"our brands\" filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110671/110671 [01:35<00:00, 1163.51it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_brands_api):\n",
    "    files_our_brands_api = glob.glob(pattern_our_brands_api)\n",
    "    print(len(files_our_brands_api))\n",
    "    \n",
    "    data_our_brands = []\n",
    "    with Pool(processes=32) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.process_our_brands_api, \n",
    "                                               files_our_brands_api), \n",
    "                           total=len(files_our_brands_api)):\n",
    "            data_our_brands.extend(record)\n",
    "\n",
    "    df_brands_api = pd.DataFrame(data_our_brands)\n",
    "    df_brands_api.drop_duplicates(subset=['asin', 'search_term'], inplace=True)\n",
    "    df_brands_api.to_csv(fn_brands_api, index=False, compression='gzip')\n",
    "    \n",
    "else:\n",
    "    df_brands_api = pd.read_csv(fn_brands_api, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our brands HTML\n",
    "parses HTML pages that are filtered to \"our brands.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48709/48709 [02:12<00:00, 366.58it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_brands_filter):\n",
    "    files_brands_filter = glob.glob(pattern_our_brands_html)\n",
    "    print(len(files_brands_filter))\n",
    "    \n",
    "    brands_filter_data = []\n",
    "    with Pool(processes=64) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.process_our_brands_filter, \n",
    "                                               files_brands_filter), \n",
    "                           total=len(files_brands_filter)):\n",
    "            brands_filter_data.extend(record)\n",
    "    df_brands_filter = pd.DataFrame(brands_filter_data)\n",
    "    df_brands_filter.drop_duplicates(subset=['asin', 'search_term'], inplace=True)\n",
    "    df_brands_filter.to_csv(fn_brands_filter, index=False, compression='gzip')\n",
    "    \n",
    "else:\n",
    "    df_brands_filter = pd.read_csv(fn_brands_filter, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Sellers\n",
    "Find best-selling Amazon devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 3256/15660 [00:05<00:12, 987.72it/s] "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_best_sellers):\n",
    "    files_best_sellers = glob.glob(pattern_best_sellers)\n",
    "    print(len(files_best_sellers))\n",
    "    \n",
    "    best_sellers_data = []\n",
    "    with Pool(processes=32) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.process_best_sellers, \n",
    "                                               files_best_sellers), \n",
    "                           total=len(files_best_sellers)):\n",
    "            best_sellers_data.extend(record)\n",
    "    df_best_sellers = pd.DataFrame(best_sellers_data)\n",
    "    df_best_sellers.to_csv(fn_best_sellers, index=False, compression='gzip')\n",
    "\n",
    "else:\n",
    "    df_best_sellers = pd.read_csv(fn_best_sellers, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon products database\n",
    "Creating the database of Amazon ASINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_missed = '|'.join([\n",
    "    'amazon basics',\n",
    "    'amazon essentials',\n",
    "    'amazon exclusive',\n",
    "    'amazon us exclusive',\n",
    "    'amazon brand',\n",
    "    'goodthreads',\n",
    "    'solimo',\n",
    "    'whole foods market',\n",
    "    'amazon commercial',\n",
    "    'amazon collection',\n",
    "    'amazon fresh',\n",
    "    'amazon elements',\n",
    "    'amazonbasics',\n",
    "    'pinzon by amazon',\n",
    "    'simple joys by carter',\n",
    "    'daily ritual',\n",
    "    '365 everyday value',\n",
    "    'lark & ro',\n",
    "    'presto!'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_amazon):\n",
    "    # Amazon products from the datasets we collected\n",
    "    df_best_seller_amazon = df_best_sellers[\n",
    "        (df_best_sellers.path.str.contains('Amazon Devices')) &\n",
    "        (~df_best_sellers.category.astype(str).str.contains('Kindle'))\n",
    "    ]\n",
    "    s1 = df_brands_api[['asin', 'product_name', 'product_url']]\n",
    "    s1 = s1.assign(source=\"our brands API\")\n",
    "    s2 = df_best_seller_amazon[['asin', 'product_name', 'product_url']]\n",
    "    s2 = s2.assign(source='best selling Amazon devices')\n",
    "    s3 =  df_brands_search[['asin', 'product_name', 'product_url']]\n",
    "    s3 = s3.assign(source='our brands searchbar')\n",
    "    s4 = df_brands_filter[['asin', 'product_name', 'product_url']]\n",
    "    s4 = s4.assign(source='our brands filtered search result')\n",
    "    df_amazon = s1.append(s2).append(s3).append(s4)\n",
    "    \n",
    "    # missed from non-search listing products\n",
    "    s5 = df_search[\n",
    "        (df_search.is_featured_brand) & \n",
    "        (df_search.product_type == 'regular_placement') &\n",
    "        (~df_search.asin.isin(df_amazon.asin))\n",
    "    ][['asin', 'product_name', 'product_url']]\n",
    "    s5 = s5.assign(source=\"featured from our brands tag\")\n",
    "    s6 = df_search[\n",
    "        (df_search.product_type == 'featured_brands_carousel') &\n",
    "        (~df_search.asin.isin(df_amazon.asin))\n",
    "    ][['asin', 'product_name', 'product_url']]\n",
    "    s6 = s6.assign(source=\"featured from our brands carousel\")\n",
    "    s7 =  df_search[\n",
    "        (df_search.product_name.str.contains(amazon_missed, case=False)) &\n",
    "        (~df_search.asin.isin(df_amazon.asin))\n",
    "    ][['asin', 'product_name', 'product_url']]\n",
    "    s7 = s7.assign(source=\"text search\")\n",
    "    df_amazon = df_amazon.append(s5).append(s6).append(s7)\n",
    "    df_amazon.drop_duplicates(subset=['asin'], keep='first', inplace=True)\n",
    "    \n",
    "    # add urls in the dataframe\n",
    "    df_amazon['product_url'] = df_amazon.product_url.apply(lambda x: 'https://www.amazon.com' + x)\n",
    "    df_amazon.to_csv(fn_amazon, index=False, compression='xz')\n",
    "else:\n",
    "    df_amazon = pd.read_csv(fn_amazon, compression='xz')\n",
    "amazon_asin = set(df_amazon.asin.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts(df_amazon, col='source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204855/204855 [20:51<00:00, 163.74it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_products):\n",
    "    files_products = glob.glob(pattern_products)\n",
    "    print(len(files_products))\n",
    "    \n",
    "    product_data = []\n",
    "    with Pool(processes=64) as pool:\n",
    "        for record in tqdm(pool.imap_unordered(P.parse_product_page, files_products), \n",
    "                           total=len(files_products)):\n",
    "            product_data.append(record)\n",
    "    df_product = pd.DataFrame(product_data)\n",
    "    \n",
    "    # create some columns for ads\n",
    "    df_product['n_ads'] = df_product.ads.apply(lambda x: len(x))\n",
    "    df_product[\"asin\"] = df_product.fn.apply(lambda x: x.split('/2021')[0].split('/')[-1])\n",
    "    \n",
    "    # dedupe and filter to only products in the search.\n",
    "    df_product.drop_duplicates(subset='asin', keep='last', inplace=True)\n",
    "    df_product = df_product[df_product.asin.isin(df_search.asin)]\n",
    "    \n",
    "    # create boolean column if the asin is an Amazon product\n",
    "    amazon_asin = set(df_amazon.asin.unique())\n",
    "    df_product['is_amazon'] = df_product.asin.isin(amazon_asin)\n",
    "    \n",
    "    # create boolean column for Amazon's shippers and sellers\n",
    "    df_product[\"is_sold_by_amazon\"] = df_product.apply(P.is_sold_by_amazon, axis=1)\n",
    "    df_product[\"is_shipped_by_amazon\"] = df_product.apply(P.is_shipped_by_amazon, axis=1)\n",
    "    df_product.to_csv(fn_products, index=False, compression='xz')\n",
    "    \n",
    "else:\n",
    "    df_product = pd.read_csv(fn_products, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Amazon.com                 38649\n",
       "AmazonFresh                 1481\n",
       "Whole Foods Market          1045\n",
       "Pharmapacks                  309\n",
       "Amazon.com Services LLC      246\n",
       "shein online store           165\n",
       "Zappos                       150\n",
       "Songmics Direct              141\n",
       "VM Express                   138\n",
       "SweatyRocks                  131\n",
       "EPFamily Direct              129\n",
       "BestChoiceproducts           124\n",
       "JoyinDirect                  122\n",
       "Just Love Fashion            119\n",
       "MYBATTERYSUPPLIER            108\n",
       "PajamaGram                   107\n",
       "iServe                       105\n",
       "Mr. Pen                      104\n",
       "TheNewMall                    96\n",
       "Baleaf Sports                 92\n",
       "Name: sold_by, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product.sold_by.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranining set for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-6ba951e921e2>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_set['top_clicked'] = training_set['asin'].isin(top_clicked)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_training_set): \n",
    "    # filter the searches to regular placements with at least 20 products\n",
    "    amazon_searches = df_search[\n",
    "        (df_search.product_type == 'regular_placement') &\n",
    "        (df_search.asin.isin(df_amazon.asin)) \n",
    "    ].search_term.unique()\n",
    "    qualified_searches = df_search[\n",
    "        (df_search.product_order >= 4) &\n",
    "        (df_search.search_term.isin(amazon_searches))\n",
    "    ].search_term.unique()\n",
    "    \n",
    "    training_set = df_search[\n",
    "        (df_search.product_type == 'regular_placement') & \n",
    "        (df_search.search_term.isin(qualified_searches))\n",
    "    ]\n",
    "    \n",
    "    # create column\n",
    "    df_seller_central = pd.read_csv(fn_seller_central, skiprows=1, index_col=0, compression='xz')\n",
    "    top_clicked = list(set(\n",
    "        df_seller_central['#1 Clicked ASIN'].unique().tolist() + \n",
    "        df_seller_central['#2 Clicked ASIN'].unique().tolist() +\n",
    "        df_seller_central['#3 Clicked ASIN'].unique().tolist()\n",
    "    ))\n",
    "    training_set['top_clicked'] = training_set['asin'].isin(top_clicked)\n",
    "    \n",
    "    # merge product metadata\n",
    "    training_set = training_set.merge(df_product[[\n",
    "        'asin','is_amazon', 'is_sold_by_amazon', \n",
    "        'is_shipped_by_amazon', 'has_third_party_sellers'\n",
    "    ]], how='left')\n",
    "    training_set.to_csv(fn_training_set, index=False, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
